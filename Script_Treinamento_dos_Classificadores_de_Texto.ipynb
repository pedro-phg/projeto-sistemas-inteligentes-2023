{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFw2j0XxHimr",
        "outputId": "3d0b78a3-b751-4fad-cf81-d6a0ea22a38c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweet_text(text:str) -> str:\n",
        "    \"\"\"\n",
        "    Limpa e pré-processa um tweet.\n",
        "\n",
        "    Argumentos:\n",
        "    text: string - Uma string que contém o tweet a ser limpo.\n",
        "\n",
        "    Retorna:\n",
        "    string - Uma string que contém o tweet pré-processado.\n",
        "    \"\"\"\n",
        "    # Define as expressões regulares para encontrar e substituir menções (@) e URLs\n",
        "    pat1 = r'@[A-Za-z0-9]+'\n",
        "    pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "    combined_pat = r'|'.join((pat1, pat2))\n",
        "    tok = WordPunctTokenizer()\n",
        "\n",
        "    # Converte os caracteres HTML em texto usando BeautifulSoup\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "\n",
        "    # Remove menções e URLs\n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "\n",
        "    try:\n",
        "        # Remove caracteres especiais\n",
        "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "\n",
        "    # Remove caracteres que não são letras do alfabeto\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "\n",
        "    # Converte todas as letras para minúsculas\n",
        "    lower_case = letters_only.lower()\n",
        "\n",
        "    # Separa as palavras\n",
        "    words = tok.tokenize(lower_case)\n",
        "    \n",
        "    # Remover stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "        \n",
        "    # Unir as palavras pré-processadas em uma única string\n",
        "    tweet = ' '.join(words)\n",
        "\n",
        "    # Retorna a string com as palavras pré-processadas\n",
        "    return (\" \".join(words)).strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "J9pGmSuvIIom"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 01: Sentiment140"
      ],
      "metadata": {
        "id": "FVxXWrkmIP6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_dataset_01 = ''"
      ],
      "metadata": {
        "id": "PVtJSJO4IUhi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path_to_dataset_01)\n",
        "\n",
        "n = len(df)\n",
        "\n",
        "# Dividindo o dataframe em dois com base no target\n",
        "positive_df = df[df['category'] == 4]\n",
        "negative_df = df[df['category'] == 0]\n",
        "\n",
        "positive_samples = positive_df.sample(n=n, replace=True)\n",
        "negative_samples = negative_df.sample(n=n, replace=True)\n",
        "\n",
        "# Combinando as amostras em um único dataframe\n",
        "df_sample = pd.concat([positive_samples, negative_samples])\n",
        "\n",
        "X = df_sample['cleaned_text']\n",
        "y = df_sample['category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                    ('tfidf', TfidfTransformer()),\n",
        "                    ('clf', LinearSVC())])\n",
        "\n",
        "text_clf_01.fit(X_train, y_train)\n",
        "\n",
        "y_pred = text_clf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sn.heatmap(cm, cmap='Blues', annot=True, fmt='g')\n",
        "\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "0LCYgQyoIRpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 02: Tweet Emotions"
      ],
      "metadata": {
        "id": "vx2pfzH-IoCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_dataset_02 = ''"
      ],
      "metadata": {
        "id": "LGjRBqr2Iux8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path_to_dataset_02)\n",
        "\n",
        "# Dividindo o dataframe em dois com base no target\n",
        "positive_df = df[df['category'] == 4]\n",
        "negative_df = df[df['category'] == 0]\n",
        "\n",
        "positive_samples = positive_df.sample(n=n, replace=True)\n",
        "negative_samples = negative_df.sample(n=n, replace=True)\n",
        "\n",
        "# Combinando as amostras em um único dataframe\n",
        "df_sample = pd.concat([positive_samples, negative_samples])\n",
        "\n",
        "X = df_sample['cleaned_text']\n",
        "y = df_sample['category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                  ('tfidf', TfidfTransformer()),\n",
        "                  ('clf', LinearSVC())])\n",
        "\n",
        "text_clf_02.fit(X_train, y_train)\n",
        "\n",
        "y_pred = text_clf.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sn.heatmap(cm, cmap='Blues', annot=True, fmt='g')\n",
        "\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(cm)\n",
        "\n"
      ],
      "metadata": {
        "id": "7kB2Qdf0ImOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testes"
      ],
      "metadata": {
        "id": "UJn8Bc2UJBwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Paste Any Text Here'\n",
        "text_clf_01.predict([preprocess_tweet_text(text)])[0]\n",
        "text_clf_02.predict([preprocess_tweet_text(text)])[0]"
      ],
      "metadata": {
        "id": "ec-7za6XJCak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}